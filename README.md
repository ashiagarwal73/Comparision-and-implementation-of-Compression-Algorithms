# Comparision-and-implementation-of-Compression-Algorithms
In todayâ€™s world, With the advent of the Internet and mobile devices with limited resources and
with the growing requirements of information storage and data transfer, Cloud Computing has
become an important aspect, but cloud computing also require physical infrastructure,
somewhere down the lane. This exponential sub purge of data leads to high demand for data
processing that leads to a high computational requirement which is usually not available at the
user's end. Compression reduces the redundancy in data representation thus increasing
effective data density.Data Compression is a technique which is used to decrease the size
of data. This is very useful when some huge files have to be transferred over networks or being
stored on a data storage device and the size is more than the capacity of the data storage or
would consume so much bandwidth for transmission in a network. With the limited physical
infrastructure for storage, data compression has gained even more importance these days.
There are number of data compression algorithms, which are dedicated to compressing
different data formats. Even for a single data type, there are number of different compression
algorithms, which use different approaches. In this project, we will examine lossless data
compression algorithms like Huffman encoding algorithm, Lempel-Ziv-Welch algorithm, and
Shannon-Fano algorithm and comparing their performance.
### To run the project:</br>
1.Open command prompt</br>
2.run `gcc main.c -lm`</br>
3.Enter input text file to be compressed.</br>
